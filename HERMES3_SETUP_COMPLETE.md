# Hermes-3-Llama-3.1-8B Setup Complete ‚úÖ

## üì¶ What's Been Done

### 1. Model Files Downloaded
- **Location**: `/tmp/hermes3-model/Hermes-3-Llama-3.1-8B-q4f16_1-MLC/`
- **Total files**: 115 files (8.4 GB)
- **Components**:
  - ‚úÖ WASM file: `Hermes-3-Llama-3.1-8B-q4f16_1-sw4k_cs1k-webgpu.wasm` (4.9 MB)
  - ‚úÖ Tokenizer files: `tokenizer.json` (8.7 MB), `tokenizer_config.json` (54 KB)
  - ‚úÖ Model shards: 108 `.bin` files (params_shard_0 through params_shard_107)
  - ‚úÖ Config files: `mlc-chat-config.json`, `ndarray-cache.json`, `tensor-cache.json`

### 2. Code Updated
- ‚úÖ Updated `blocks/ko-chat-assistant/ko-chat-assistant-llm.js`
  - Model ID: `Hermes-3-Llama-3.1-8B-q4f16_1-MLC`
  - Model size: 8.5 GB
  - Path: `/models/Hermes-3-Llama-3.1-8B-q4f16_1-MLC/`
  - WASM: `Hermes-3-Llama-3.1-8B-q4f16_1-sw4k_cs1k-webgpu.wasm`
- ‚úÖ Updated comments to reflect Hermes-3 instead of Qwen2.5
- ‚úÖ Updated console logs with accurate expectations (85-95% tool calling accuracy)

---

## üöÄ Next Steps: Upload to R2

### **Step 1: Upload All Files to R2 Bucket**

You need to upload the entire `/tmp/hermes3-model/Hermes-3-Llama-3.1-8B-q4f16_1-MLC/` directory to your R2 bucket.

**R2 Bucket URL**: `https://pub-0945642565ac4afc9e23f2a0ffcbd46e.r2.dev/`

**Target Path**: Upload all 115 files to:
```
/models/Hermes-3-Llama-3.1-8B-q4f16_1-MLC/
```

**Files to Upload** (all 115 files):
- `Hermes-3-Llama-3.1-8B-q4f16_1-sw4k_cs1k-webgpu.wasm` (4.9 MB)
- `tokenizer.json` (8.7 MB)
- `tokenizer_config.json` (54 KB)
- `params_shard_0.bin` through `params_shard_107.bin` (108 files, ~8 GB total)
- `mlc-chat-config.json` (2.4 KB)
- `ndarray-cache.json` (145 KB)
- `tensor-cache.json` (145 KB)
- `README.md` (1.5 KB)

**Upload Method** (same as before):
- Use Cloudflare R2 Dashboard: https://dash.cloudflare.com/ ‚Üí R2 ‚Üí Your bucket ‚Üí Upload
- OR use Rclone/AWS CLI if you have it configured

---

### **Step 2: Verify Files Are Accessible**

After uploading, verify the Worker proxy can access the files:

```bash
# Test WASM file
curl -I https://mcp-server-koassets.adobeaem.workers.dev/models/Hermes-3-Llama-3.1-8B-q4f16_1-MLC/Hermes-3-Llama-3.1-8B-q4f16_1-sw4k_cs1k-webgpu.wasm

# Test tokenizer
curl -I https://mcp-server-koassets.adobeaem.workers.dev/models/Hermes-3-Llama-3.1-8B-q4f16_1-MLC/tokenizer.json

# Test model shard
curl -I https://mcp-server-koassets.adobeaem.workers.dev/models/Hermes-3-Llama-3.1-8B-q4f16_1-MLC/params_shard_0.bin
```

All should return `HTTP/2 200` with correct content lengths.

---

### **Step 3: Deploy Code Changes**

Commit and push the code changes:

```bash
cd /Users/pkoch/Documents/GitHub/koassets
git add blocks/ko-chat-assistant/ko-chat-assistant-llm.js
git commit -m "Switch to Hermes-3-Llama-3.1-8B for improved tool calling (85-95% accuracy)"
git push origin mcp-server
```

Wait for Cloudflare Pages to deploy (usually 1-2 minutes).

---

### **Step 4: Test in Browser**

1. **Clear Browser Cache & IndexedDB**:
   - Open DevTools ‚Üí Application ‚Üí Storage ‚Üí Clear site data
   - This removes cached Qwen2.5 model (5 GB)

2. **Open the Chat**:
   ```
   https://mcp-server-koassets.adobeaem.workers.dev/chat
   ```

3. **Watch the Model Load**:
   - First load: Will download 8.4 GB (8-12 minutes)
   - Console should show:
     ```
     [WebLLM] Creating MLCEngine with Hermes-3-Llama-3.1-8B
     [WebLLM] Model ID: Hermes-3-Llama-3.1-8B-q4f16_1-MLC
     [WebLLM] Source: SELF-HOSTED via R2 + Worker Proxy
     [WebLLM] NOTE: Hermes-3 is the newest version with best tool calling accuracy (85-95%)
     [WebLLM] Progress: Start to fetch params
     [WebLLM] Fetching param cache[0/108]: ...
     ```

4. **Test a Simple Query**:
   ```
   Please search assets with 'summer' as search term
   ```

5. **Check Console for Tool Calling**:
   ```
   [WebLLM] üß† MODEL THINKING:
   [WebLLM] üîß Tool calls requested by model:
   [WebLLM]   1. Tool name: "search_assets"
   [WebLLM]      Arguments (parsed): {
     "query": "summer",
     "facetFilters": {},
     "hitsPerPage": 12
   }
   [WebLLM] ‚úì Valid tool call detected: search_assets
   ```

---

## üìä Expected Improvements Over Hermes-2-Pro-Mistral

| Metric | Hermes-2-Pro-Mistral (Old) | Hermes-3-Llama-3.1 (New) |
|--------|---------------------------|--------------------------|
| **Tool Calling Accuracy** | üî¥ 25% (tested) | üü¢ 85-95% (expected) |
| **Hallucinated Tool Names** | üî¥ 75% of requests | üü¢ 5-10% of requests |
| **Context Window** | 4k tokens | 8k tokens (2x) |
| **Model Size** | 7B parameters | 8B parameters |
| **Download Size** | 4.0 GB | 8.4 GB |
| **Base Model** | Mistral-7B (2023) | Llama-3.1-8B (2024) |
| **Hermes Version** | v2 (older) | v3 (newest) |

---

## üéØ Success Criteria

After deployment, Hermes-3 should:
1. ‚úÖ **Use correct tool name**: Always call `search_assets` (not "Summer", "Video", etc.)
2. ‚úÖ **Correct parameter types**: `facetFilters` as object `{}` (not array)
3. ‚úÖ **Proper query usage**: Keywords in `query`, filters in `facetFilters`
4. ‚úÖ **High success rate**: 85-95% of requests should work correctly
5. ‚úÖ **Multi-turn conversations**: Better context retention with 8k window

---

## üêõ If Issues Occur

### CORS Errors
- **Symptom**: `No 'Access-Control-Allow-Origin' header`
- **Fix**: Worker proxy should be handling this. Check Worker logs.

### 404 on Model Files
- **Symptom**: `GET /models/Hermes-3.../params_shard_X.bin 404`
- **Fix**: Verify files uploaded to R2 with correct path: `/models/Hermes-3-Llama-3.1-8B-q4f16_1-MLC/`

### Still Loading Qwen2.5
- **Symptom**: Console shows "Qwen2.5-7B-Instruct"
- **Fix**: Hard refresh (Cmd+Shift+R) and clear IndexedDB

### Tool Calling Still Fails
- **Symptom**: Still hallucinating tool names after Hermes-3 loads
- **Action**: Share console logs - there might be a prompting issue

---

## üìÅ Files to Keep vs. Remove

### Keep These:
- ‚úÖ `/tmp/hermes3-model/` - Backup in case re-upload needed
- ‚úÖ `test-hermes3-config.html` - Useful for debugging

### Can Remove Later (After Testing):
- `/tmp/qwen25-model/` - Qwen2.5 is not supported for tool calling
- `test-qwen-config.html` - No longer needed

---

## üéâ Next Action

**‚Üí Upload the 115 files from `/tmp/hermes3-model/Hermes-3-Llama-3.1-8B-q4f16_1-MLC/` to R2 bucket under `/models/Hermes-3-Llama-3.1-8B-q4f16_1-MLC/`**

Then:
1. Verify with `curl -I` commands above
2. Commit & push code changes
3. Test in browser
4. Report results!

Good luck! üöÄ

